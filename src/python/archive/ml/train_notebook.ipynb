{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter nbconvert models.ipynb --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import pickle\n",
    "\n",
    "import losswise\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from datasets import BurstDataset, ShuffledBatchSequentialSampler, FakeBurstDataset\n",
    "from prep_dataset import BurstDatasetStandardizer\n",
    "from models import Encoder, Decoder\n",
    "from train_functions import *\n",
    "from eval_functions import plot_autoencoding, autoencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement cuda stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and prep datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_BURST_SECS, MAX_BURST_SECS = 0.1, 3\n",
    "MIN_EPISODE_MINS, MAX_EPISODE_MINS = 10, None\n",
    "\n",
    "PAD_LENGTH = 3*200 # corresponds to 5 sec long burst\n",
    "BATCH_BY_LEN = True\n",
    "\n",
    "DATA_DIR = '/home/alice-eeg/NFS/script_output/describe_bs/'\n",
    "MAX_NUM_PATIENTS = 40\n",
    "dataset = BurstDataset(min_burst_secs=MIN_BURST_SECS, max_burst_secs=MAX_BURST_SECS, \n",
    "                       min_episode_mins=MIN_EPISODE_MINS, max_episode_mins=MAX_EPISODE_MINS, \n",
    "                       sort_len=False)\n",
    "dataset.init_dataset(DATA_DIR, PAD_LENGTH, max_num_patients=MAX_NUM_PATIENTS)\n",
    "train_split, dev_split = 0.6, 0.2\n",
    "train_dataset, dev_dataset, test_dataset = dataset.split(train_split, dev_split, split_sort_len=BATCH_BY_LEN)\n",
    "\n",
    "standardizer = BurstDatasetStandardizer()\n",
    "standardizer.fit_transform(train_dataset)\n",
    "standardizer.transform(dev_dataset)\n",
    "standardizer.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "INPUT_SIZE = 1 # This CANNOT be changed! \n",
    "BIDIRECTIONAL = True\n",
    "NUM_LAYERS = 1\n",
    "EXTRA_INPUT_DIM = False\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS)\n",
    "decoder = Decoder(HIDDEN_SIZE, INPUT_SIZE, extra_input_dim=EXTRA_INPUT_DIM, encoder_bidirectional=BIDIRECTIONAL, \n",
    "                  num_layers=NUM_LAYERS)\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30 \n",
    "NUM_EPOCHS = 100 # Normally use 50, but can stop early at 20\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "TEACHER_FORCING_SLOPE = 0.001\n",
    "TRAIN_REVERSED = True\n",
    "\n",
    "SAVE_DIR = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_sampler = ShuffledBatchSequentialSampler(dataset, batch_size=40, drop_last=False)\n",
    "# data_loader = torch.utils.data.DataLoader(dataset, batch_sampler=batch_sampler)\n",
    "# for batch in data_loader:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LOSSWISE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    # dataset filtering\n",
    "    'min_burst_secs':MIN_BURST_SECS, 'max_burst_secs':MAX_BURST_SECS, \n",
    "    'min_episode_mins':MIN_EPISODE_MINS, 'max_episode_mins':MAX_EPISODE_MINS, \n",
    "    # dataset size\n",
    "    'max num patients':MAX_NUM_PATIENTS, 'len(train_data)':len(train_dataset), \n",
    "    'train split':train_split, 'dev split':dev_split, \n",
    "    # dataset padding\n",
    "    'pad length': PAD_LENGTH, \n",
    "    # model params\n",
    "    'hidden size': HIDDEN_SIZE, 'bidirectional':BIDIRECTIONAL, 'num layers':NUM_LAYERS, \n",
    "    'extra input dim':EXTRA_INPUT_DIM, \n",
    "    # training params\n",
    "    'batch by length': BATCH_BY_LEN, 'batch size':BATCH_SIZE, 'num epochs': NUM_EPOCHS, \n",
    "    'learning rate': LR, 'weight decay': WEIGHT_DECAY, \n",
    "    'teacher forcing slope': TEACHER_FORCING_SLOPE, 'train reversed':TRAIN_REVERSED, \n",
    "    'save dir': SAVE_DIR}\n",
    "if SAVE_DIR is not None:\n",
    "    pickle.dump(params_dict, open(os.path.join(SAVE_DIR, \"params_dict.pkl\"), \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LOSSWISE:\n",
    "    losswise.set_api_key('W2TAMB3SZ') # api_key for \"coma-eeg\"\n",
    "    session = losswise.Session(tag='Run with num layers 3', max_iter=NUM_EPOCHS,\n",
    "                               params=params_dict)\n",
    "    losswise_graph = session.graph('loss', kind='min')\n",
    "else:\n",
    "    losswise_graph = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(train_dataset, dev_dataset, test_dataset, encoder, decoder, SAVE_DIR,\n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            batch_size=BATCH_SIZE, lr=LR, weight_decay=WEIGHT_DECAY, \n",
    "            teacher_forcing_slope=TEACHER_FORCING_SLOPE, train_reversed=TRAIN_REVERSED, batch_by_len=BATCH_BY_LEN, \n",
    "            losswise_graph=losswise_graph, params_dict=params_dict)\n",
    "session.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the autoencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset[8]\n",
    "#mse = plot_autoencoding(sample, encoder, decoder, toss_encoder_output=False, reverse=TRAIN_REVERSED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
