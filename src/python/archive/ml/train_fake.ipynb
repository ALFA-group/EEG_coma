{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter nbconvert models.ipynb --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import losswise\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "\n",
    "from datasets import BurstDataset, ShuffledBatchSequentialSampler, FakeBurstDataset\n",
    "from prep_dataset import BurstDatasetStandardizer\n",
    "from models import Encoder, Decoder\n",
    "from train_functions import *\n",
    "from eval_functions import plot_autoencoding, autoencode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: implement cuda stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp = 1\n",
    "phase_shift = None\n",
    "freq = None\n",
    "noise_std = 0.01\n",
    "PAD_LENGTH = 500\n",
    "dataset_len = 1000\n",
    "test_dataset_len = 50\n",
    "train_dataset = FakeBurstDataset(amp=amp, phase_shift=phase_shift, freq=freq, noise_std=noise_std, dataset_len=dataset_len, pad_len=PAD_LENGTH)\n",
    "dev_dataset = FakeBurstDataset(amp=amp, phase_shift=phase_shift, freq=freq, noise_std=noise_std, dataset_len=test_dataset_len, pad_len=PAD_LENGTH)\n",
    "test_dataset = FakeBurstDataset(amp=amp, phase_shift=phase_shift, freq=freq, noise_std=noise_std, dataset_len=test_dataset_len, pad_len=PAD_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "INPUT_SIZE = 1 # This CANNOT be changed! \n",
    "BIDIRECTIONAL = True\n",
    "NUM_LAYERS = 1\n",
    "EXTRA_INPUT_DIM = False\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS)\n",
    "decoder = Decoder(HIDDEN_SIZE, INPUT_SIZE, extra_input_dim=EXTRA_INPUT_DIM, encoder_bidirectional=BIDIRECTIONAL, \n",
    "                  num_layers=NUM_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30 \n",
    "NUM_EPOCHS = 60 # Normally use 50, but can stop early at 20\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0\n",
    "# rate at which the probability of teacher_forcing decreases. \n",
    "TEACHER_FORCING_SLOPE = 0.01   # set this to None to turn off teacher forcing.\n",
    "TRAIN_REVERSED = True\n",
    "\n",
    "SAVE_DIR = None\n",
    "\n",
    "BATCH_BY_LEN = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### params_dict for fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LOSSWISE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {'pad length': PAD_LENGTH, 'noise_std': noise_std,  \n",
    "               'amp': amp, 'freq':freq, 'phase shift': phase_shift,\n",
    "         'train dataset len':len(train_dataset), 'test dataset len':len(test_dataset), \n",
    "               'hidden size': HIDDEN_SIZE, 'batch size':BATCH_SIZE, \n",
    "               'bidirectional':BIDIRECTIONAL, 'num layers':NUM_LAYERS, \n",
    "               'extra input dim':EXTRA_INPUT_DIM,\n",
    "         'num epochs': NUM_EPOCHS, 'learning rate': LR, 'weight decay': WEIGHT_DECAY, \n",
    "          'teacher forcing slope': TEACHER_FORCING_SLOPE, 'train reversed':TRAIN_REVERSED, 'save dir': SAVE_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LOSSWISE:\n",
    "    losswise.set_api_key('W2TAMB3SZ') # api_key for \"coma-eeg\"\n",
    "    session = losswise.Session(tag='Testing autoencoder with fake data', max_iter=NUM_EPOCHS,\n",
    "                               params=params_dict)\n",
    "    losswise_graph = session.graph('loss', kind='min')\n",
    "else:\n",
    "    losswise_graph = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(train_dataset, dev_dataset, test_dataset, encoder, decoder, SAVE_DIR,\n",
    "            num_epochs=NUM_EPOCHS, \n",
    "            batch_size=BATCH_SIZE, lr=LR, weight_decay=WEIGHT_DECAY, \n",
    "            teacher_forcing_slope=TEACHER_FORCING_SLOPE, train_reversed=TRAIN_REVERSED, batch_by_len=BATCH_BY_LEN, \n",
    "            losswise_graph=losswise_graph, params_dict=params_dict)\n",
    "if USE_LOSSWISE:\n",
    "    session.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the autoencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample = test_dataset[38]\n",
    "mse = plot_autoencoding(sample, encoder, decoder, toss_encoder_output=False, reverse=TRAIN_REVERSED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "works = {'amp': 1,\n",
    " 'batch size': 20,\n",
    " 'freq': 1,\n",
    " 'hidden size': 100,\n",
    " 'learning rate': 0.001,\n",
    " 'noise_std': 0.01,\n",
    " 'num epochs': 20,\n",
    " 'pad length': 300,\n",
    " 'phase shift': None,\n",
    " 'save dir': None,\n",
    " 'teacher forcing': True,\n",
    " 'test dataset len': 50,\n",
    " 'train dataset len': 200,\n",
    " 'weight decay': 0}\n",
    "\n",
    "works = {'amp': 1,\n",
    " 'batch size': 10,\n",
    " 'bidirectional': True,\n",
    " 'extra input dim': False,\n",
    " 'freq': None,\n",
    " 'hidden size': 100,\n",
    " 'learning rate': 0.001,\n",
    " 'noise_std': 0.01,\n",
    " 'num epochs': 40,\n",
    " 'num layers': 1,\n",
    " 'pad length': 300,\n",
    " 'phase shift': None,\n",
    " 'save dir': None,\n",
    " 'teacher forcing slope': 0.15,\n",
    " 'test dataset len': 50,\n",
    " 'train dataset len': 600,\n",
    " 'train reversed': True,\n",
    " 'weight decay': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
