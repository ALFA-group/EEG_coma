{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import losswise\n",
    "\n",
    "from prettytable import PrettyTable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from datasets import BurstDataset, ShuffledBatchSequentialSampler, FakeBurstDataset\n",
    "from prep_dataset import BurstDatasetStandardizer\n",
    "from models import Encoder, Decoder\n",
    "from eval_functions import plot_autoencoding, autoencode, encode\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "from readers.patient_info import PatientInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = 'saved_encs/config18/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = pickle.load(open(os.path.join(SAVE_DIR, 'params_dict.pkl')))\n",
    "(train_dataset, dev_dataset, test_dataset) = pickle.load(open(os.path.join(SAVE_DIR, 'datasets.pkl')))\n",
    "standardizer = pickle.load(open(os.path.join(SAVE_DIR, 'standardizer.pkl')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = params_dict['hidden_size']\n",
    "INPUT_SIZE = 1 # This CANNOT be changed! \n",
    "BIDIRECTIONAL = params_dict['bidirectional']\n",
    "NUM_LAYERS = params_dict['num_layers']\n",
    "EXTRA_INPUT_DIM = params_dict['extra_input_dim']\n",
    "\n",
    "encoder = Encoder(INPUT_SIZE, HIDDEN_SIZE, bidirectional=BIDIRECTIONAL, num_layers=NUM_LAYERS)\n",
    "decoder = Decoder(HIDDEN_SIZE, INPUT_SIZE, extra_input_dim=EXTRA_INPUT_DIM, encoder_bidirectional=BIDIRECTIONAL, \n",
    "                  num_layers=NUM_LAYERS)\n",
    "if torch.cuda.is_available():\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = params_dict['num_epochs']\n",
    "use_epoch = 278 # by defualt, use the last epoch\n",
    "encoder.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'epoch{}_enc.pkl'.format(use_epoch))))\n",
    "decoder.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'epoch{}_dec.pkl'.format(use_epoch))))\n",
    "encoder.eval()\n",
    "decoder.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the autoencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_REVERSED = params_dict['train reversed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsample_factor = params_dict['downsample_factor']\n",
    "robust = params_dict['robust_scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "dataset = test_dataset\n",
    "for i in range(len(dataset)-15, len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    try:\n",
    "        # for updated datasets\n",
    "        undownsampled = dataset.get_undownsampled_item(i, standardizer, robust)\n",
    "    except AttributeError:\n",
    "        print('old dataset')\n",
    "        # for old datasets\n",
    "        undownsampled = None\n",
    "    mse = plot_autoencoding(sample, encoder, decoder, toss_encoder_output=False, \n",
    "                            reverse=TRAIN_REVERSED, undownsampled=undownsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot encodings in 2d for all patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.figure_factory\n",
    "from plotly.graph_objs import *\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_and_cat(bursts, masks):\n",
    "    out, hidden, cell = encode(bursts, masks, encoder)\n",
    "    catted_hidden = torch.cat([hidden[i, :, :] for i in range(hidden.size(0))], dim=1)\n",
    "    catted_cell = torch.cat([cell[i, :, :] for i in range(hidden.size(0))], dim=1)\n",
    "    encodings = torch.cat([catted_hidden, catted_cell], dim=1)\n",
    "    return encodings.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_encodings(indices_list, labels, pca, max_points_per_label=5):\n",
    "    # indices_list is list of lists of indices, where each list is a group of indices with the same label\n",
    "    # labels is list of labels \n",
    "    traces = []\n",
    "    for inds, label in zip(indices_list, labels):\n",
    "        bursts = np.take(test_dataset.all_bursts, inds, axis=0)\n",
    "        masks = np.take(test_dataset.all_burst_masks, inds, axis=0)\n",
    "        burst_info = [test_dataset.all_burst_info[i] for i in inds]\n",
    "        X = encode_and_cat(bursts, masks)\n",
    "        print X.shape\n",
    "        #X = StandardScaler().fit_transform(X)\n",
    "        X_after_pca = pca.transform(X)\n",
    "        if max_points_per_label is not None and X_after_pca.shape[0] > max_points_per_label:\n",
    "            # if there's too many points, use kmeans to get summary points\n",
    "            kmeans = KMeans(n_clusters=max_points_per_label, random_state=0).fit(X_after_pca)\n",
    "            summary_points = kmeans.cluster_centers_\n",
    "        else:\n",
    "            summary_points = X_after_pca\n",
    "        trace = Scatter(\n",
    "            x=summary_points[:,0],\n",
    "            y=summary_points[:,1],\n",
    "            mode='markers',\n",
    "            name=label,\n",
    "            marker=Marker(\n",
    "                size=12,\n",
    "                line=Line(\n",
    "                    color='rgba(217, 217, 217, 0.14)',\n",
    "                    width=0.5),\n",
    "                opacity=0.8))\n",
    "        traces.append(trace)\n",
    "    data = Data(traces)\n",
    "    layout = Layout(xaxis=XAxis(title='PC1', showline=False),\n",
    "                    yaxis=YAxis(title='PC2', showline=False))\n",
    "    fig = Figure(data=data, layout=layout)\n",
    "    plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataset = BurstDataset('/home/alice-eeg/NFS/script_output/describe_bs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patientInfo = PatientInfo('../../../patient_outcome_info/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset.all_bursts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = IncrementalPCA(n_components=2)\n",
    "all_encodings = []\n",
    "batch_size = 10\n",
    "data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "for batch in tqdm(data_loader):\n",
    "    masks = batch['mask']\n",
    "    bursts = batch['burst']\n",
    "    out, hidden, cell = encode(bursts, masks, encoder)\n",
    "    batch_encodings = encode_and_cat(masks, bursts)\n",
    "    pca.partial_fit(batch_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts = set([])\n",
    "for burst_info in test_dataset.all_burst_info:\n",
    "    edf, episode_start_ind, episode_end_ind, burst_num = newDataset.parse_burst_info(burst_info)\n",
    "    pt = utils.get_pt_from_edf_name(edf)\n",
    "    pts.add(pt)\n",
    "indices_list = []\n",
    "labels = []\n",
    "for pt in pts:\n",
    "    inds = [i for (i, val) in enumerate(test_dataset.all_burst_info) if '{}_'.format(pt) in val]\n",
    "    inds = np.array(inds)\n",
    "    indices_list.append(inds)\n",
    "    labels.append(pt)\n",
    "plot_encodings(indices_list, labels, pca)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
