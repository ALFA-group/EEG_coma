{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "jupyter nbconvert clean_patient_info.ipynb --to script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import utils\n",
    "from utils import df_to_dict, filter_by_pt, read_list\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "duration_file = '../../../patient_outcome_info/eeg_data_size_info.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "durations = pd.read_csv(open(duration_file), delimiter='\\t')\n",
    "assert(all(durations['srate']==200))\n",
    "assert(all(durations['nchans']==19))\n",
    "durations = durations.drop(labels=['srate', 'nchans'], axis=1)\n",
    "old_edf_to_nsamples = df_to_dict(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exclude_patients = read_list('../../../patient_outcome_info/exclude_patient_sids.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_to_new_map = pd.read_csv('../../../patient_outcome_info/old_to_new_id.csv')\n",
    "old_to_new_map.columns = ['old_id', 'new_id']\n",
    "old_to_new_id = df_to_dict(old_to_new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_to_new_edf = pd.read_csv('../../../patient_outcome_info/old_to_new_edf_mapping.csv')\n",
    "columns = list(old_to_new_edf.columns)\n",
    "columns[0] = 'pt_edf_id'\n",
    "old_to_new_edf.columns = columns\n",
    "old_to_new_edf = old_to_new_edf.drop(labels=['pt_edf_id', 'CSAIL_id', 'new_id', 'error', 'icare_edfs', 'master_edfs',\n",
    "                                             'exclude'], axis=1)\n",
    "old_to_new_edf = df_to_dict(old_to_new_edf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# invert to create new to old id mapping\n",
    "new_to_old_id = {}\n",
    "for old in old_to_new_id:\n",
    "    new = old_to_new_id[old]\n",
    "    if new in new_to_old_id:\n",
    "        new_to_old_id[new].append(old)\n",
    "    else:\n",
    "        new_to_old_id[new] = [old]\n",
    "new_to_old_edf = {}\n",
    "for old in old_to_new_edf:\n",
    "    new = old_to_new_edf[old]\n",
    "    if new in new_to_old_edf:\n",
    "        new_to_old_edf[new].append(old)\n",
    "    else:\n",
    "        new_to_old_edf[new] = [old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_sid: list of timestamps for new sid -> map to old edfs -> duration, is timestamp guessed?, start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_info_list = []\n",
    "patients_with_gaps = []\n",
    "for new_pt_name in sorted(new_to_old_id.iterkeys(), key=utils.parse_patient_name):\n",
    "    pt_merge_info_list = []\n",
    "    new_sid = utils.convert_new_pname_to_new_sid(new_pt_name)\n",
    "    if new_sid in exclude_patients:\n",
    "        continue\n",
    "    old_edfs = filter_by_pt(old_to_new_edf.keys(), new_to_old_id[new_pt_name])\n",
    "#     new_edfs = filter_by_pt(new_to_old_edf.keys(), new_pt_name)\n",
    "#     new_for_old = [old_to_new_edf[old] for old in old_edfs]\n",
    "#     new_for_old = [new for new in new_for_old if not pd.isnull(new)]\n",
    "#     assert(set(new_for_old)==set(new_edfs))\n",
    "    for old_edf in old_edfs:\n",
    "        new_edf = old_to_new_edf[old_edf]\n",
    "        if pd.isnull(new_edf):\n",
    "            start_datetime = utils.parse_edf_name(old_edf, ts_datetime=True)[-1]\n",
    "            timestamp_guessed = True\n",
    "        else:\n",
    "            start_datetime = utils.parse_edf_name(new_edf, ts_datetime=True)[-1]\n",
    "            timestamp_guessed = False\n",
    "        nsamples = old_edf_to_nsamples[old_edf]\n",
    "        duration = utils.samples_to_hour(nsamples)\n",
    "        pt_merge_info_list.append([new_sid, start_datetime, old_edf, duration, nsamples, timestamp_guessed, new_edf])\n",
    "    pt_merge_info_list.sort()\n",
    "    for i, pt_merge_info in enumerate(pt_merge_info_list):\n",
    "        new_sid, start_datetime, old_edf, duration, nsamples, timestamp_guessed, new_edf = pt_merge_info\n",
    "        end_datetime_obj = start_datetime + timedelta(seconds=utils.samples_to_sec(nsamples))\n",
    "        end_timestamp = utils.convert_datetime_to_timestamp(end_datetime_obj)\n",
    "        timestamp = utils.convert_datetime_to_timestamp(start_datetime)\n",
    "        if i==len(pt_merge_info_list)-1:\n",
    "            time_to_next = '-'\n",
    "        else:\n",
    "            next_start_datetime = pt_merge_info_list[i+1][1]\n",
    "            time_to_next = (next_start_datetime - end_datetime_obj).total_seconds()/3600.\n",
    "            if time_to_next > 24 or time_to_next < 0:\n",
    "                patients_with_gaps.append(new_sid)\n",
    "        merge_info_list.append([new_sid, timestamp, end_timestamp, old_edf, duration, \n",
    "                                nsamples, time_to_next, timestamp_guessed, new_edf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_info_df = pd.DataFrame(merge_info_list, columns=['sid', 'timestamp', 'end_timestamp', 'csail_edf_name', \n",
    "                                                       'duration_(hours)', 'nsamples', 'time_to_next_edf_(hours)',\n",
    "                                                       'timestamp_guessed', 'new_edf_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_info_df.to_csv('../../../patient_outcome_info/merge_info_w_new_edf_name.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merge_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('../../../patient_outcome_info/merge_info.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# analyze gaps\n",
    "\n",
    "time_to_next = merge_info_df['time_to_next_edf_(hours)']\n",
    "time_to_next = time_to_next[time_to_next != '-']\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n",
    "    print time_to_next.sort_values()\n",
    "merge_info_df.loc[merge_info_df['sid'].isin(patients_with_gaps)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below is old, unused stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for old_pt_name in old_to_new_id.keys()[-5:]:\n",
    "    if pd.isnull(old_pt_name):\n",
    "        print 'null'\n",
    "    new_pt_name = old_to_new_id[old_pt_name]\n",
    "    if new_pt_name in exclude_patients:\n",
    "        continue\n",
    "    old_edfs = filter_by_pt(old_to_new_edf.keys(), old_pt_name)\n",
    "    timestamps = []\n",
    "    for old_edf in old_edfs:\n",
    "        new_edf = old_to_new_edf[old_edf]\n",
    "        if pd.isnull(new_edf):\n",
    "            new_edf = old_edf\n",
    "        timestamps.append(utils.parse_edf_name(new_edf, ts_datetime=True)[-1])\n",
    "    edf_nsamples = [durations[old] for old in old_edfs]\n",
    "    edf_nsecs = [timedelta(seconds=nsamp/200.) for nsamp in edf_nsamples]\n",
    "    edf_end_time = [nsecs+ts for (ts, nsecs) in zip(timestamps, edf_nsecs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timestamps, edf_end_time\n",
    "list1, list2 = zip(*sorted(zip(timestamps, edf_end_time)))\n",
    "start_time_sorted, end_time_sorted = list(list1), list(list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_to_old_id = {}\n",
    "for old in old_to_new_id:\n",
    "    new = old_to_new_id[old]\n",
    "    if new in new_to_old_id:\n",
    "        new_to_old_id[new].append(old)\n",
    "    else:\n",
    "        new_to_old_id[new] = [old]\n",
    "new_to_old_edf = {}\n",
    "for old in old_to_new_edf:\n",
    "    new = old_to_new_edf[old]\n",
    "    if new in new_to_old_edf:\n",
    "        new_to_old_edf[new].append(old)\n",
    "    else:\n",
    "        new_to_old_edf[new] = [old]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new in new_to_old_id:\n",
    "    if len(new_to_old_id[new])>1:\n",
    "        if utils.convert_new_pname_to_new_sid(new) in exclude_patients:\n",
    "            #print 'exclude', new, new_to_old_id[new]\n",
    "            continue\n",
    "        print new, new_to_old_id[new]\n",
    "# old 210 and 1636 combine to new 210\n",
    "# old 211 and 1605 are duplicates\n",
    "# old 213 and 1641 combine to form 213\n",
    "# old bi 32 and bi 100 combine to form bi 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for new in new_to_old_edf:\n",
    "    if pd.isnull(new):\n",
    "        continue\n",
    "    if len(new_to_old_edf[new])>1:\n",
    "        olds = new_to_old_edf[new]\n",
    "        if (durations[olds[0]]!=durations[olds[1]]):\n",
    "            print new, olds, durations[olds[0]], durations[olds[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "old_to_new_id_csv = '../../../patient_outcome_info/old_to_new_id.csv'\n",
    "exclude_list = '../../../patient_outcome_info/exclude_patient_sids.txt'\n",
    "class PatientInfoReader:\n",
    "    def __init__(old_to_new_id_csv='../../../patient_outcome_info/old_to_new_id.csv',\n",
    "                 exclude_list='../../../patient_outcome_info/exclude_patient_sids.txt',\n",
    "                 old_to_new_edf_csv='../../../patient_outcome_info/old_to_new_edf_mapping.csv',\n",
    "                 outcome_csv='../../../patient_outcome_info/clinical_outcomes.csv'\n",
    "                ):\n",
    "        # exclude\n",
    "        self.exclude_patients = read_list(exclude_list)\n",
    "        \n",
    "        # old to new ids\n",
    "        old_to_new_map = pd.read_csv(old_to_new_id_csv)\n",
    "        old_to_new_map.columns = ['old_id', 'new_id']\n",
    "        self.old_to_new_id = df_to_dict(old_to_new_map)\n",
    "        \n",
    "        # old to new edfs\n",
    "        old_to_new_edf = pd.read_csv(old_to_new_edf_csv)\n",
    "        columns = list(old_to_new_edf.columns)\n",
    "        columns[0] = 'pt_edf_id'\n",
    "        old_to_new_edf.columns = columns\n",
    "        old_to_new_edf = old_to_new_edf.drop(columns=['pt_edf_id', 'CSAIL_id', 'new_id', 'error', 'icare_edfs', 'master_edfs',\n",
    "                                                     'exclude'])\n",
    "        self.old_to_new_edf = df_to_dict(old_to_new_edf)\n",
    "        \n",
    "        # new id to outcomes\n",
    "        self.outcomes_df = pd.read_csv(outcome_csv, index_col=0)\n",
    "\n",
    "    def is_excluded(self, patient_filename):\n",
    "        patient_sid = self.standardize_pt_name(patient_filename)\n",
    "        return patient_sid in self.exclude_sids\n",
    "        \n",
    "    def has_good_outcome(self, patient_filename, vb=False):\n",
    "        outcome = self.get_outcome(patient_filename, vb)\n",
    "        if outcome==-1:\n",
    "            return outcome\n",
    "        return self.is_good_outcome(outcome)\n",
    "    \n",
    "    def is_good_outcome(self, outcome):\n",
    "        return is_good_outcome(outcome)\n",
    "    \n",
    "    def is_bad_outcome(self, outcome):\n",
    "        return is_bad_outcome(outcome)\n",
    "    \n",
    "    def get_patient_info(self, patient_filename):\n",
    "        patient_sid = self.standardize_pt_name(patient_filename)\n",
    "        return self.outcomes_df.loc[patient_sid]\n",
    "    \n",
    "    def get_outcome(self, patient_filename, vb=False):\n",
    "        patient_sid = self.standardize_pt_name(patient_filename)\n",
    "        try:\n",
    "            outcome = self.outcomes_df['bestCpcBy6Mo'][patient_sid]\n",
    "        except KeyError:\n",
    "            if vb:\n",
    "                if patient_sid in self.exclude_sids:\n",
    "                    print('{} excluded'.format(patient_filename))\n",
    "                else:\n",
    "                    print('KeyError on {}'.format(patient_filename))\n",
    "            return -1\n",
    "        if np.isnan(outcome):\n",
    "            if vb:\n",
    "                if patient_sid in self.exclude_sids:\n",
    "                    print('{} excluded'.format(patient_filename))\n",
    "                else:\n",
    "                    print('Isnan outcome on {}'.format(patient_filename))\n",
    "            return -1\n",
    "        return outcome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
